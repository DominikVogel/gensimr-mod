% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/similarity.R
\name{similarity}
\alias{similarity}
\alias{similarity.gensim.corpora.mmcorpus.MmCorpus}
\alias{similarity.mm_file}
\alias{similarity.python.builtin.tuple}
\title{Similarity}
\usage{
similarity(corpus, ...)

\method{similarity}{gensim.corpora.mmcorpus.MmCorpus}(corpus, num_features,
  ...)

\method{similarity}{mm_file}(corpus, num_features, ...)

\method{similarity}{python.builtin.tuple}(corpus, num_features, ...)
}
\arguments{
\item{corpus}{A corpus.}

\item{...}{Any other parameters to pass to the Python function, see 
\href{https://radimrehurek.com/gensim/similarities/docsim.html#gensim.similarities.docsim.Similarity}{official documentation}.}

\item{num_features}{Size of the dictionary i.e.:\code{reticulate::py_len(dictionary)}.}
}
\description{
Splits the index into several smaller sub-indexes (“shards”), 
which are disk-based. If your entire index fits in memory 
(~one million documents per 1GB of RAM), you can also use 
the \code{\link{similarity_matrix}}. It is more simple but
does not scale as well: it keeps the entire index in RAM, 
no sharding. It also do not support adding new document 
to the index dynamically.
}
