---
output: 
  github_document:
    html_preview: false
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)

reticulate::use_virtualenv("./env")
```

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
[![Travis build status](https://travis-ci.org/news-r/gensimr.svg?branch=master)](https://travis-ci.org/news-r/gensimr)
<!-- badges: end -->

![gensim official logo](https://radimrehurek.com/gensim/_static/images/gensim.png)

Brings [gensim](https://radimrehurek.com/gensim) to R: efficient large-scale topic modeling.

⚠️ Notice the "Experimental" lifecycle badge: things won't work, stuff will break.

- [Installation](#installation)
- [Preprocessing](#preprocessing)
- [Topic Modeling](#topic-modeling)
- [Document Similarity](#document-similarity)

## Installation

Install the package.

```r
# install.packages("remotes")
remotes::install_github("news-r/gensimr")
```

Install the python dependency. 

```r
gensimr::install_gensim()
```

Ideally one should use a virtual environment and pass it to `install_gensim`, only do this once.

```r
# replace with path of your choice
my_env <- "./env"

# run this (works on unix)
args <- paste("-m venv", env)
system2("python3", args) # create environment
reticulate::use_virtualenv(my_env) # force reticulate to use env
gensimr::install_gensim(my_env) # install gensim in environment
```

## Preprocessing

First we preprocess the corpus using example data, a tiny corpus of 9 documents. Reproducing the tutorial on [corpora and vector spaces](https://radimrehurek.com/gensim/tut1.html).

```{r}
library(gensimr)

data(corpus, package = "gensimr")

print(corpus)

docs <- preprocess(corpus)
```

Once preprocessed we can build a dictionary.

```{r}
dictionary <- corpora_dictionary(docs)
```

A dictionary essentially assigns an integer to each term.

`doc2bow` simply applies the method of the same name to every documents (see example below); it counts the number of occurrences of each distinct word, converts the word to its integer word id and returns the result as a sparse vector. 

```{r}
# native method to a single document
dictionary$doc2bow(docs[[1]])

# apply to all documents
corpus_bow <- doc2bow(dictionary, docs)
```

Then serialise to matrix market format, the function returns the path to the file (this is saved on disk for efficiency), if no path is passed then a temp file is created. Here we set `auto_delete` to `FALSE` otherwise the corpus is deleted after first use. Note this means you should manually delete it with `delete_mmcorpus`.

```{r}
(corpus_mm <- serialize_mmcorpus(corpus_bow, auto_delete = FALSE))
```

Then initialise a model, we're going to use a Latent Similarity Indexing method later on (`model_lsi`) which requires td-idf.

```{r}
tfidf <- model_tfidf(corpus_mm)
```

We can then use the model to transform our original corpus.

```{r}
corpus_transformed <- wrap(tfidf, corpus_bow)
```

## Topic Modeling

Finally, we can build models, the number of topics of `model_*` functions defautls to 2, which is too low for what we generally would do with gensimr but works for the low number of documents we have. Below we reproduce bits and bobs of the [topics and transformation](https://radimrehurek.com/gensim/tut2.html).

### Latent Similarity Index

Note that we use the transformed corpus.

```{r}
lsi <- model_lsi(corpus_transformed, id2word = dictionary)
lsi$print_topics()
```

We can then wrap the model around the corpus to extract further information, below we extract how each document contribute to each dimension (topic).

```{r}
wrapped_corpus <- wrap(lsi, corpus_transformed)
(wrapped_corpus_docs <- get_docs_topics(wrapped_corpus))
plot(wrapped_corpus_docs$dimension_1_y, wrapped_corpus_docs$dimension_2_y)
```

### Random Projections

Note that we use the transformed corpus.

```{r}
rp <- model_rp(corpus_transformed, id2word = dictionary)

wrapped_corpus <- wrap(rp, corpus_transformed)
wrapped_corpus_docs <- get_docs_topics(wrapped_corpus)
plot(wrapped_corpus_docs$dimension_1_y, wrapped_corpus_docs$dimension_2_y)
```

### Latent Dirichlet Allocation

Note that we use the original, non-transformed corpus.

```{r}
lda <- model_lda(corpus_mm, id2word = dictionary)
lda_topics <- lda$get_document_topics(corpus_bow)
wrapped_corpus_docs <- get_docs_topics(lda_topics)
plot(wrapped_corpus_docs$dimension_1_y, wrapped_corpus_docs$dimension_2_y)
```

### Hierarchical Dirichlet Process

```{r}
hdp <- model_hdp(corpus_mm, id2word = dictionary)
reticulate::py_to_r(hdp$show_topic(topic_id = 1L, topn = 5L))
```

## Document Similarity

Reproducing [tutorial on similarity](https://radimrehurek.com/gensim/tut3.html#similarity-interface).

```{r}
mm <- read_serialized_mmcorpus(corpus_mm)

new_document <- "A human and computer interaction"
preprocessed_new_document <- preprocess(new_document, min_freq = 0)
vec_bow <- doc2bow(dictionary, preprocessed_new_document)
vec_lsi <- wrap(lsi, vec_bow)

wrapped_lsi <- wrap(lsi, mm)
index <- similarity_matrix(wrapped_lsi)

sims <- wrap(index, vec_lsi)

get_similarity(sims)
```

Clean up, delete the corpus.

```r
delete_mmcorpus(corpus_mm)
```