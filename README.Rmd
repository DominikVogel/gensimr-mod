---
output: 
  github_document:
    html_preview: false
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)

reticulate::use_virtualenv("./env")
```

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
[![Travis build status](https://travis-ci.org/news-r/gensimr.svg?branch=master)](https://travis-ci.org/news-r/gensimr)
<!-- badges: end -->

# gensimr

Brings [gensim](https://radimrehurek.com/gensim) to R: efficient large-scale topic modeling.

## Installation

Install the package.

```r
# install.packages("remotes")
remotes::install_github("news-r/gensimr")
```

Install the python dependency. 

```r
gensimr::install_gensim()
```

Ideally one should use a virtual environment and pass it to `install_gensim`, only do this once.

```r
my_env <- "./env"
args <- paste("-m venv", env)
system2("python3", args) # create environment
reticulate::use_virtualenv(my_env) # force reticulate to use env
gensimr::install_gensim(my_env) # install gensim in environment
```

## Example

First we preprocess the corpus using example data, a tiny corpus of 9 documents.

```{r}
library(gensimr)

data(corpus, package = "gensimr")

docs <- preprocess(corpus)
```

Once preprocessed we can build a dictionary.

```{r}
dictionary <- corpora_dictionary(docs)
```

A dictionary essentially assigns an integer to each term.

```{r}
reticulate::py_to_r(dictionary$token2id)
```

`doc2bow` simply applies the method of the same name to every documents (see example below); it counts the number of occurrences of each distinct word, converts the word to its integer word id and returns the result as a sparse vector. 

```{r}
# native method to a single document
dictionary$doc2bow(docs[[1]])

# apply to all documents
corpus <- doc2bow(dictionary, docs)
```

Then convert to matrix market format and serialise, the function returns the path to the file (this is saved on disk for efficiency), if no path is passed then a temp file is created.

```{r}
(mm_corpus <- mmcorpus_serialize(corpus))
```

Then initialise a model, we're going to use a Latent Similarity Indexing method later on (`model_lsi`) which requires td-idf.

```{r}
model <- model_tfidf(mm_corpus)
```

We can then use the model to transform our original corpus.

```{r}
corpus_transformed <- corpora_transform(model, corpus)
```

Finally, we build the model, the number of topics defautls to 2.

```{r}
lsi <- model_lsi(corpus_transformed, dictionary)
lsi$print_topics()
```

We can then wrap the model around the corpus to extract further information, below we extract how each document contribute to each dimension (topic).

```{r}
wrapped_corpus <- wrap_corpus(lsi, corpus_transformed)
(wrapped_corpus_docs <- wrap_corpus_docs(wrapped_corpus))
```