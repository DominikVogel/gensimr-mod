---
output: 
  github_document:
    html_preview: false
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)

reticulate::use_virtualenv("./env")
```

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
[![Travis build status](https://travis-ci.org/news-r/gensimr.svg?branch=master)](https://travis-ci.org/news-r/gensimr)
<!-- badges: end -->

# gensimr

Brings [gensim](https://radimrehurek.com/gensim) to R: efficient large-scale topic modeling.

⚠️ Notice the "Experimental" lifecycle badge: things won't work, stuff will break.

## Installation

Install the package.

```r
# install.packages("remotes")
remotes::install_github("news-r/gensimr")
```

Install the python dependency. 

```r
gensimr::install_gensim()
```

Ideally one should use a virtual environment and pass it to `install_gensim`, only do this once.

```r
# replace with path of your choice
my_env <- "./env"

# run this (works on unix)
args <- paste("-m venv", env)
system2("python3", args) # create environment
reticulate::use_virtualenv(my_env) # force reticulate to use env
gensimr::install_gensim(my_env) # install gensim in environment
```

## Topics

First we preprocess the corpus using example data, a tiny corpus of 9 documents.

```{r}
library(gensimr)

data(corpus, package = "gensimr")

print(corpus)

docs <- preprocess(corpus)
```

Once preprocessed we can build a dictionary.

```{r}
dictionary <- corpora_dictionary(docs)
```

A dictionary essentially assigns an integer to each term.

`doc2bow` simply applies the method of the same name to every documents (see example below); it counts the number of occurrences of each distinct word, converts the word to its integer word id and returns the result as a sparse vector. 

```{r}
# native method to a single document
dictionary$doc2bow(docs[[1]])

# apply to all documents
corpus_bow <- doc2bow(dictionary, docs)
```

Then convert to matrix market format and serialise, the function returns the path to the file (this is saved on disk for efficiency), if no path is passed then a temp file is created.

```{r}
(corpus_mm <- mmcorpus_serialize(corpus_bow))
```

Then initialise a model, we're going to use a Latent Similarity Indexing method later on (`model_lsi`) which requires td-idf.

```{r}
tfidf <- model_tfidf(corpus_mm)
```

We can then use the model to transform our original corpus.

```{r}
corpus_transformed <- wrap(tfidf, corpus_bow)
```

Finally, we can build models, the number of topics of `model_*` functions defautls to 2, which is too low for what we generally would do with gensimr but works for the low number of documents we have.

### Latent Similarity Index

Note that we use the transformed corpus.

```{r}
lsi <- model_lsi(corpus_transformed, id2word = dictionary)
lsi$print_topics()
```

We can then wrap the model around the corpus to extract further information, below we extract how each document contribute to each dimension (topic).

```{r}
wrapped_corpus <- wrap(lsi, corpus_transformed)
(wrapped_corpus_docs <- get_docs_topics(wrapped_corpus))
plot(wrapped_corpus_docs$dimension_1_y, wrapped_corpus_docs$dimension_2_y)
```

### Random Projections

Note that we use the transformed corpus.

```{r}
rp <- model_rp(corpus_transformed, id2word = dictionary)

wrapped_corpus <- wrap(rp, corpus_transformed)
wrapped_corpus_docs <- get_docs_topics(wrapped_corpus)
plot(wrapped_corpus_docs$dimension_1_y, wrapped_corpus_docs$dimension_2_y)
```

### Latent Dirichlet Allocation

Note that we use the original, non-transformed corpus.

```{r}
corpus_mm <- mmcorpus_serialize(corpus_bow)
lda <- model_lda(corpus_mm, id2word = dictionary)
lda_topics <- lda$get_document_topics(corpus_bow)
wrapped_corpus_docs <- get_docs_topics(lda_topics)
plot(wrapped_corpus_docs$dimension_1_y, wrapped_corpus_docs$dimension_2_y)
```

## Similarity

```{r}
corpus_mm <- mmcorpus_serialize(corpus_bow)
mm <- read_serialized_mmcorpus(corpus_mm)

new_document <- "A human and computer interaction"
preprocessed_new_document <- preprocess(new_document, min_freq = 0)
vec_bow <- doc2bow(dictionary, preprocessed_new_document)
vec_lsi <- wrap(lsi, vec_bow)

wrapped_lsi <- wrap(lsi, mm)
index <- similarity_matrix(wrapped_lsi)

sims <- wrap(index, vec_lsi)

get_similarity(sims)
```